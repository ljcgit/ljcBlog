## 1.创建索引过程中，定制分析器

可以定义为零个或者多个字符过滤器、一个分词器、零个或多个分词过滤器。

```nhttp
curl -XPOST ip:9200/index

{
	"settings" : {
		"number_of_shards" : 2,
		"number_of_replicas" : 1,
		"index" : {
				"analysis" : {
						"analyzer" : {
								"myCustomAnalyzer" : { 
										"type" : "custom",         //定制化的类型
										"tokenizer" : "myCustomTokernizer",     //分词
										"filter" : ["myCustomFilter1","muCustomFilter2"],    //分词过滤器
										"char_filter" : ["myCustomCharFilter"]					//字符过滤器
								}
						},
						"tokenizer" : {       //分词器
								"myCustomTokenizer1" : {
										"type" : "letter"
								}
						},
						"filter" : {					//过滤器		
								"myCustomFilter1" : {
										"type" : "lowercase"
								},
								"myCustomTokenizer2" : {
										"type" : "kstem"
								}
						},
						"char_filter" : {			字符过滤器
								"myCustomCharFilter" : {
										"type" : "mapping",
										"mappings" : ["ph=>f","u=>you"]
								}
						}
				}
		},
		"mappings" : {
					
		}
	}
}
```





## 2.在映射中指定某个字段的分析器

```json
{
   "mappings" : {
     	"document" : {
        	"properties" : {
            	"description" : {
									"type" : "string",
                	"analyzer" : "myCustomAnalyzer"
              }
          }
      }
   }
}
```

#### 2.1 指定字段不被分析

指定index字段为not_analyzed。

```json
{
   "mappings" : {
     	"document" : {
        	"properties" : {
            	"name" : {
									"type" : "string",
                	"index" : "not_analyzed"
              }
          }
      }
   }
}
```

#### 2.2 使用多字段类型来存储分析方式不同的文本

```json
{
   "mappings" : {
     	"document" : {
        	"properties" : {
            	"name" : {
									"type" : "string",
                	"index" : "standard",
                	"fields" : {
                    	"raw" : {
													"index" : "not_analyzed",   //字段的原始版本，没有经过分析
                        	"type"  : "string"
                      }
                  }
              }
          }
      }
   }
}
```





## 3.使用API分析文本

```http
ip:9200/_analyze?analyzer=standard&text=hello word
```

#### 3.1 自定义分析器

```http
ip:9200/_analyze?tokenizer=whitespace&filters=lowercase,reverse&text=hello world
```

#### 3.2 基于字段的分析

```http
ip:9200/index/_analyze?field=description&text=hello world
```

#### 3.3 词条向量API

```http
ip:9200/index/type/{id}/_termvector?pretty=true
```





## 4.内置分词器

#### 4.1 标准分词器

基于语法的分词器，也移除了逗号和句号这样的标点符号。

> tokenizer=standard

#### 4.2 关键词分词器

将整个文本作为单个的分词。

> tokenizer=keyword

#### 4.3 字母分词器

根据非字母的符号，将文本切分成分词。

> tokenizer=letter

#### 4.4 小写分词器

结合了常规的字母分词器和小写分词过滤器。

> tokenizer=lowercase

#### 4.5 空白分词器

通过空白来分隔不同的分词，包括空格、制表符、换行。这种分词器不会删除任何标点符号。

> tokenizer=whitespace

#### 4.6 模式分词器

指定一个任意的模式，将文本切分为分词。

在文本 .-. 的地方将分词断开。

```http
{
	"settings" : {
		"index" : {
			"analysis" : {
				"tokenizer" : {
					"pattern1" : {
						"type" : "pattern",
						"pattern" : "\\.-\\."
					}
				}
			}
		}
	}
}
```

#### 4.7 UAX URL 电子邮件分词器

> tokenizer=uax_url_email

#### 4.8 路径层次分词器

> tokenizer=path_hierarchy



## 5.内置分词过滤器

#### 5.1 小写分词过滤器

将任何进过的分词转换为小写。

> filter=lowercase

#### 5.2 长度分词过滤器

将长度超出最短和最长限制范围的单词过滤掉。

```http
ip:9200/length

{
	"settings" : {
		"index" : {
			"analysis" : {
				"filter" : {
					"my-length-filter" : {
						"type" : "length",
						"max" : 8,
						"min" : 2
					}
				}
			}
		}
	}
}
```

#### 5.3 停用词分词过滤器

将停用词从分词流中移除。

> 英文默认的停用词列表：
>
> a,an,and,are,as,at,be,but,by,for,if,in,into,is,it,no,not,of,on,or,such,that,the,their,then,there,these,they,this,to,was,will,with

```json
"filter" : {
  "my-stop-filter" : {
    	"type" : "stop",
    	"stopwords" : ["the","a","an"]
  }
}
```

使用配置文件的停用词过滤器，每个单词应该在新的一行上:

```json
"filter" : {
  "my-stop-filter" : {
    	"type" : "stop",
    	"stopwords_path" : "config/stopwords.txt"
  }
}
```

#### 5.4 截断分词过滤器(truncate token filter)

通过设置定制配置中的length参数，截断超过一定长度的分词。默认截断多于10个字符的部分。

#### 5.5 修剪分词过滤器(trim token filter)

删除一个分词中的所有空白部分。

#### 5.6 限制分词数量分词过滤器(limit token count token filter)

限制某个字段可包含分词的最大数量。使用max_token_count参数，默认是1。

#### 5.7  颠倒分词过滤器

颠倒每个分词。

> filter=reverse

#### 5.8 唯一分词过滤器

只保留唯一的分词，保留第一个匹配根刺的元数据，而将其后出现的重复删除。

> filter=unique

#### 5.9 ASCII折叠分词过滤器

将不是普通的ASCII字符的Unicode字符转化为ASCII中等同的字符，前提是这种等同存在。

> filter=asciifolding

#### 5.10 同义词分词过滤器

在分词流中的同样位移处，使用关键词的同义词取代原始分词。

```json
"filter" : {
  "my-synonym-filter" : {
    "type" : "synonym",
    "expand" : "true",
    "synonyms" : ["automobile"=>"car"]
  }
}
```



## 6.N元语法

字符级别。

```json
"filter" : {
  "ngf1" : {
    "type" : "edgeNgram",
    "min_gram" : 2,
    "max_gram" : 6
  }
}
```

## 7.滑动窗口分词过滤器

分词级别。

```json
"filter" : {
  "shingle-filter" : {
    "type" : "shingle",
    "min_shingle_size" : 2,   //最小为2
    "max_shingle_size" : 3,
    "output_unigrams" : false.   //不要保留原始的单个分词。
  
}
```



> 默认情况下，shingles过滤器包括了原始的分词。



## 8.提取词干

提取词干是将单词缩减到基本或词根的形式。

#### 8.1 算法提取词干

+ snowball
+ porter
+ kstem

#### 8.2 使用词典提取词干

当创建一个hunspell分析器的时候，字典文件应该是在名为hunspell的目录里，并且hunspell目录和elasticsearch.yml处于同一个目录中。在hunspell目录中，每种语言的字典是一个以其关联地区命名的目录。

```json
"filter" : {
  "hunFilter" : {
    "type" : "hunspell",
    "locale" : "en_US",
    "dedup" : true
  }
}
```

